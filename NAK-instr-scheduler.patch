diff --git a/src/nouveau/compiler/nak.h b/src/nouveau/compiler/nak.h
index 83f8571fb846d90e51b1b730ced4393bb0b5594a..d42e40fbc57f79b5d4a58dfc8617556be483ddcb 100644
--- a/src/nouveau/compiler/nak.h
+++ b/src/nouveau/compiler/nak.h
@@ -141,6 +141,24 @@ struct nak_shader_info {
    /** Number of instructions used */
    uint32_t num_instrs;
 
+   /** Number of cycles used by fixed-latency instructions */
+   uint32_t num_static_cycles;
+
+   /** Number of spills from GPRs to Memory */
+   uint32_t num_spills_to_mem;
+
+   /** Number of fills from Memory to GPRs */
+   uint32_t num_fills_from_mem;
+
+   /** Number of spills between register files */
+   uint32_t num_spills_to_reg;
+
+   /** Number of fills between register files */
+   uint32_t num_fills_from_reg;
+
+   /** Maximum number of warps per SM */
+   uint32_t occupancy_in_warps_per_sm;
+
    /** Size of shader local (scratch) memory */
    uint32_t slm_size;
 
diff --git a/src/nouveau/compiler/nak/api.rs b/src/nouveau/compiler/nak/api.rs
index 3d54bb9e6dfd81716867badd48b26081e866af62..1fbcfd7e4a882bb0db4fa524fcfacde66eb859b3 100644
--- a/src/nouveau/compiler/nak/api.rs
+++ b/src/nouveau/compiler/nak/api.rs
@@ -238,6 +238,12 @@ impl ShaderBin {
             num_control_barriers: info.num_control_barriers,
             _pad0: Default::default(),
             num_instrs: info.num_instrs,
+            num_static_cycles: info.num_static_cycles,
+            num_spills_to_mem: info.num_spills_to_mem,
+            num_fills_from_mem: info.num_fills_from_mem,
+            num_spills_to_reg: info.num_spills_to_reg,
+            num_fills_from_reg: info.num_fills_from_reg,
+            occupancy_in_warps_per_sm: info.occupancy_in_warps_per_sm,
             slm_size: info.slm_size,
             crs_size: sm.crs_size(info.max_crs_depth),
             __bindgen_anon_1: match &info.stage {
@@ -312,6 +318,10 @@ impl ShaderBin {
 
             eprintln!("Stage: {}", stage_name);
             eprintln!("Instruction count: {}", c_info.num_instrs);
+            eprintln!("Static cycle count: {}", c_info.num_static_cycles);
+            eprintln!("Spills to mem: {}", c_info.num_spills_to_mem);
+            eprintln!("Spills to reg: {}", c_info.num_spills_to_reg);
+            eprintln!("Occupancy (warps/SM): {}", c_info.occupancy_in_warps_per_sm);
             eprintln!("Num GPRs: {}", c_info.num_gprs);
             eprintln!("SLM size: {}", c_info.slm_size);
 
@@ -416,6 +426,7 @@ pub extern "C" fn nak_compile_shader(
     pass!(s, opt_dce);
     pass!(s, opt_out);
     pass!(s, legalize);
+    pass!(s, opt_instr_sched_prepass);
     pass!(s, assign_regs);
     pass!(s, lower_par_copies);
     pass!(s, lower_copy_swap);
@@ -427,6 +438,7 @@ pub extern "C" fn nak_compile_shader(
 
     s.remove_annotations();
 
+    pass!(s, opt_instr_sched_postpass);
     pass!(s, calc_instr_deps);
 
     s.gather_info();
diff --git a/src/nouveau/compiler/nak/assign_regs.rs b/src/nouveau/compiler/nak/assign_regs.rs
index 149ce0a1778a5573439cae1e55e3403b8b6d03fc..194689f9b70361677323675435ec184531163303 100644
--- a/src/nouveau/compiler/nak/assign_regs.rs
+++ b/src/nouveau/compiler/nak/assign_regs.rs
@@ -1419,7 +1419,7 @@ impl Shader<'_> {
         for file in spill_files {
             let num_regs = self.sm.num_regs(file);
             if max_live[file] > num_regs {
-                f.spill_values(file, num_regs);
+                f.spill_values(file, num_regs, &mut self.info);
 
                 // Re-calculate liveness after we spill
                 live = SimpleLiveness::for_function(f);
@@ -1461,7 +1461,7 @@ impl Shader<'_> {
             total_gprs = max_gprs;
             gpr_limit = total_gprs - u32::from(tmp_gprs);
 
-            f.spill_values(RegFile::GPR, gpr_limit);
+            f.spill_values(RegFile::GPR, gpr_limit, &mut self.info);
 
             // Re-calculate liveness one last time
             live = SimpleLiveness::for_function(f);
diff --git a/src/nouveau/compiler/nak/calc_instr_deps.rs b/src/nouveau/compiler/nak/calc_instr_deps.rs
index caf1d4143c21a5a663fda3dfd3a7b1c902546236..7e7b603985198c746cc3eaadeb3295a8f36b0a69 100644
--- a/src/nouveau/compiler/nak/calc_instr_deps.rs
+++ b/src/nouveau/compiler/nak/calc_instr_deps.rs
@@ -3,134 +3,12 @@
 
 use crate::api::{GetDebugFlags, DEBUG};
 use crate::ir::*;
+use crate::sched_common::*;
 
 use std::cmp::max;
 use std::collections::{HashMap, HashSet};
-use std::ops::{Index, IndexMut, Range};
 use std::slice;
 
-struct RegTracker<T> {
-    reg: [T; 255],
-    ureg: [T; 63],
-    pred: [T; 7],
-    upred: [T; 7],
-    carry: [T; 1],
-}
-
-fn new_array_with<T, const N: usize>(f: &impl Fn() -> T) -> [T; N] {
-    let mut v = Vec::new();
-    for _ in 0..N {
-        v.push(f());
-    }
-    v.try_into()
-        .unwrap_or_else(|_| panic!("Array size mismatch"))
-}
-
-impl<T> RegTracker<T> {
-    pub fn new_with(f: &impl Fn() -> T) -> Self {
-        Self {
-            reg: new_array_with(f),
-            ureg: new_array_with(f),
-            pred: new_array_with(f),
-            upred: new_array_with(f),
-            carry: new_array_with(f),
-        }
-    }
-
-    pub fn for_each_instr_pred_mut(
-        &mut self,
-        instr: &Instr,
-        mut f: impl FnMut(&mut T),
-    ) {
-        if let PredRef::Reg(reg) = &instr.pred.pred_ref {
-            for i in &mut self[*reg] {
-                f(i);
-            }
-        }
-    }
-
-    pub fn for_each_instr_src_mut(
-        &mut self,
-        instr: &Instr,
-        mut f: impl FnMut(usize, &mut T),
-    ) {
-        for (i, src) in instr.srcs().iter().enumerate() {
-            match &src.src_ref {
-                SrcRef::Reg(reg) => {
-                    for t in &mut self[*reg] {
-                        f(i, t);
-                    }
-                }
-                SrcRef::CBuf(CBufRef {
-                    buf: CBuf::BindlessUGPR(reg),
-                    ..
-                }) => {
-                    for t in &mut self[*reg] {
-                        f(i, t);
-                    }
-                }
-                _ => (),
-            }
-        }
-    }
-
-    pub fn for_each_instr_dst_mut(
-        &mut self,
-        instr: &Instr,
-        mut f: impl FnMut(usize, &mut T),
-    ) {
-        for (i, dst) in instr.dsts().iter().enumerate() {
-            if let Dst::Reg(reg) = dst {
-                for t in &mut self[*reg] {
-                    f(i, t);
-                }
-            }
-        }
-    }
-}
-
-impl<T> Index<RegRef> for RegTracker<T> {
-    type Output = [T];
-
-    fn index(&self, reg: RegRef) -> &[T] {
-        let range = reg.idx_range();
-        let range = Range {
-            start: usize::try_from(range.start).unwrap(),
-            end: usize::try_from(range.end).unwrap(),
-        };
-
-        match reg.file() {
-            RegFile::GPR => &self.reg[range],
-            RegFile::UGPR => &self.ureg[range],
-            RegFile::Pred => &self.pred[range],
-            RegFile::UPred => &self.upred[range],
-            RegFile::Carry => &self.carry[range],
-            RegFile::Bar => &[], // Barriers have a HW scoreboard
-            RegFile::Mem => panic!("Not a register"),
-        }
-    }
-}
-
-impl<T> IndexMut<RegRef> for RegTracker<T> {
-    fn index_mut(&mut self, reg: RegRef) -> &mut [T] {
-        let range = reg.idx_range();
-        let range = Range {
-            start: usize::try_from(range.start).unwrap(),
-            end: usize::try_from(range.end).unwrap(),
-        };
-
-        match reg.file() {
-            RegFile::GPR => &mut self.reg[range],
-            RegFile::UGPR => &mut self.ureg[range],
-            RegFile::Pred => &mut self.pred[range],
-            RegFile::UPred => &mut self.upred[range],
-            RegFile::Carry => &mut self.carry[range],
-            RegFile::Bar => &mut [], // Barriers have a HW scoreboard
-            RegFile::Mem => panic!("Not a register"),
-        }
-    }
-}
-
 #[derive(Clone)]
 enum RegUse<T: Clone> {
     None,
@@ -463,104 +341,6 @@ fn assign_barriers(f: &mut Function, sm: &dyn ShaderModel) {
     }
 }
 
-fn exec_latency(sm: u8, op: &Op) -> u32 {
-    if sm >= 70 {
-        match op {
-            Op::Bar(_) | Op::MemBar(_) => {
-                if sm >= 80 {
-                    6
-                } else {
-                    5
-                }
-            }
-            Op::CCtl(_op) => {
-                // CCTL.C needs 8, CCTL.I needs 11
-                11
-            }
-            // Op::DepBar(_) => 4,
-            _ => 1, // TODO: co-issue
-        }
-    } else {
-        match op {
-            Op::CCtl(_)
-            | Op::MemBar(_)
-            | Op::Bra(_)
-            | Op::SSy(_)
-            | Op::Sync(_)
-            | Op::Brk(_)
-            | Op::PBk(_)
-            | Op::Cont(_)
-            | Op::PCnt(_)
-            | Op::Exit(_)
-            | Op::Bar(_)
-            | Op::Kill(_)
-            | Op::OutFinal(_) => 13,
-            _ => 1,
-        }
-    }
-}
-
-fn instr_latency(op: &Op, dst_idx: usize) -> u32 {
-    let file = match op.dsts_as_slice()[dst_idx] {
-        Dst::None => return 0,
-        Dst::SSA(vec) => vec.file().unwrap(),
-        Dst::Reg(reg) => reg.file(),
-    };
-
-    // This is BS and we know it
-    match file {
-        RegFile::GPR => 6,
-        RegFile::UGPR => 12,
-        RegFile::Pred => 13,
-        RegFile::UPred => 11,
-        RegFile::Bar => 0, // Barriers have a HW scoreboard
-        RegFile::Carry => 6,
-        RegFile::Mem => panic!("Not a register"),
-    }
-}
-
-/// Read-after-write latency
-fn raw_latency(
-    _sm: u8,
-    write: &Op,
-    dst_idx: usize,
-    _read: &Op,
-    _src_idx: usize,
-) -> u32 {
-    instr_latency(write, dst_idx)
-}
-
-/// Write-after-read latency
-fn war_latency(
-    _sm: u8,
-    _read: &Op,
-    _src_idx: usize,
-    _write: &Op,
-    _dst_idx: usize,
-) -> u32 {
-    // We assume the source gets read in the first 4 cycles.  We don't know how
-    // quickly the write will happen.  This is all a guess.
-    4
-}
-
-/// Write-after-write latency
-fn waw_latency(
-    _sm: u8,
-    a: &Op,
-    a_dst_idx: usize,
-    _b: &Op,
-    _b_dst_idx: usize,
-) -> u32 {
-    // We know our latencies are wrong so assume the wrote could happen anywhere
-    // between 0 and instr_latency(a) cycles
-    instr_latency(a, a_dst_idx)
-}
-
-/// Predicate read-after-write latency
-fn paw_latency(_sm: u8, _write: &Op, _dst_idx: usize) -> u32 {
-    13
-}
-
 fn calc_delays(f: &mut Function, sm: &dyn ShaderModel) {
     for b in f.blocks.iter_mut().rev() {
         let mut cycle = 0_u32;
diff --git a/src/nouveau/compiler/nak/from_nir.rs b/src/nouveau/compiler/nak/from_nir.rs
index f7fa2e28ea247b83ec7160bcf1a3922817a9f5be..24c47b0918f32073005576c743d1abed7437b6ad 100644
--- a/src/nouveau/compiler/nak/from_nir.rs
+++ b/src/nouveau/compiler/nak/from_nir.rs
@@ -23,6 +23,12 @@ fn init_info_from_nir(nak: &nak_compiler, nir: &nir_shader) -> ShaderInfo {
     ShaderInfo {
         num_gprs: 0,
         num_instrs: 0,
+        num_static_cycles: 0,
+        num_spills_to_mem: 0,
+        num_fills_from_mem: 0,
+        num_spills_to_reg: 0,
+        num_fills_from_reg: 0,
+        occupancy_in_warps_per_sm: 0,
         num_control_barriers: 0,
         slm_size: nir.scratch_size,
         max_crs_depth: 0,
diff --git a/src/nouveau/compiler/nak/hw_tests.rs b/src/nouveau/compiler/nak/hw_tests.rs
index c6d3b28c0dcfab07e1fed6ef03f74a2c49c82006..8f5ca0d63fc969c84bb30519e084d3183e3a7f0f 100644
--- a/src/nouveau/compiler/nak/hw_tests.rs
+++ b/src/nouveau/compiler/nak/hw_tests.rs
@@ -201,6 +201,12 @@ impl<'a> TestShaderBuilder<'a> {
             num_gprs: 0,
             num_control_barriers: 0,
             num_instrs: 0,
+            num_static_cycles: 0,
+            num_spills_to_mem: 0,
+            num_fills_from_mem: 0,
+            num_spills_to_reg: 0,
+            num_fills_from_reg: 0,
+            occupancy_in_warps_per_sm: 0,
             slm_size: 0,
             max_crs_depth: 0,
             uses_global_mem: true,
diff --git a/src/nouveau/compiler/nak/ir.rs b/src/nouveau/compiler/nak/ir.rs
index b13a9bd867e007a59e8630bb28435a6d7194230b..f68da4d6611c7dc7ac9bda05f3a1e8e0d330e3c6 100644
--- a/src/nouveau/compiler/nak/ir.rs
+++ b/src/nouveau/compiler/nak/ir.rs
@@ -300,7 +300,7 @@ impl Iterator for RegFileSet {
     }
 }
 
-#[derive(Clone, Copy)]
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub struct PerRegFile<T> {
     per_file: [T; NUM_REG_FILES],
 }
@@ -6765,6 +6765,141 @@ impl Instr {
         }
     }
 
+    pub fn is_virtual(&self) -> bool {
+        match &self.op {
+            // Float ALU
+            Op::F2FP(_)
+            | Op::FAdd(_)
+            | Op::FFma(_)
+            | Op::FMnMx(_)
+            | Op::FMul(_)
+            | Op::FSet(_)
+            | Op::FSetP(_)
+            | Op::HAdd2(_)
+            | Op::HFma2(_)
+            | Op::HMul2(_)
+            | Op::HSet2(_)
+            | Op::HSetP2(_)
+            | Op::HMnMx2(_)
+            | Op::FSwzAdd(_) => false,
+
+            // Multi-function unit
+            Op::Rro(_) | Op::MuFu(_) => false,
+
+            // Double-precision float ALU
+            Op::DAdd(_)
+            | Op::DFma(_)
+            | Op::DMnMx(_)
+            | Op::DMul(_)
+            | Op::DSetP(_) => false,
+
+            // Integer ALU
+            Op::BRev(_)
+            | Op::Flo(_)
+            | Op::PopC(_)
+            | Op::IMad(_)
+            | Op::IMul(_)
+            | Op::BMsk(_)
+            | Op::IAbs(_)
+            | Op::IAdd2(_)
+            | Op::IAdd2X(_)
+            | Op::IAdd3(_)
+            | Op::IAdd3X(_)
+            | Op::IDp4(_)
+            | Op::IMad64(_)
+            | Op::IMnMx(_)
+            | Op::ISetP(_)
+            | Op::Lop2(_)
+            | Op::Lop3(_)
+            | Op::Shf(_)
+            | Op::Shl(_)
+            | Op::Shr(_)
+            | Op::Bfe(_) => false,
+
+            // Conversions
+            Op::F2F(_) | Op::F2I(_) | Op::I2F(_) | Op::I2I(_) | Op::FRnd(_) => {
+                false
+            }
+
+            // Move ops
+            Op::Mov(_) | Op::Prmt(_) | Op::Sel(_) | Op::Shfl(_) => false,
+
+            // Predicate ops
+            Op::PLop3(_) | Op::PSetP(_) => false,
+
+            // Uniform ops
+            Op::R2UR(_) => false,
+
+            // Texture ops
+            Op::Tex(_)
+            | Op::Tld(_)
+            | Op::Tld4(_)
+            | Op::Tmml(_)
+            | Op::Txd(_)
+            | Op::Txq(_) => false,
+
+            // Surface ops
+            Op::SuLd(_) | Op::SuSt(_) | Op::SuAtom(_) => false,
+
+            // Memory ops
+            Op::Ld(_)
+            | Op::Ldc(_)
+            | Op::St(_)
+            | Op::Atom(_)
+            | Op::AL2P(_)
+            | Op::ALd(_)
+            | Op::ASt(_)
+            | Op::Ipa(_)
+            | Op::CCtl(_)
+            | Op::LdTram(_)
+            | Op::MemBar(_) => false,
+
+            // Control-flow ops
+            Op::BClear(_)
+            | Op::Break(_)
+            | Op::BSSy(_)
+            | Op::BSync(_)
+            | Op::SSy(_)
+            | Op::Sync(_)
+            | Op::Brk(_)
+            | Op::PBk(_)
+            | Op::Cont(_)
+            | Op::PCnt(_)
+            | Op::Bra(_)
+            | Op::Exit(_)
+            | Op::WarpSync(_) => false,
+
+            // Barrier
+            Op::BMov(_) => false,
+
+            // Geometry ops
+            Op::Out(_) | Op::OutFinal(_) => false,
+
+            // Miscellaneous ops
+            Op::Bar(_)
+            | Op::CS2R(_)
+            | Op::Isberd(_)
+            | Op::Kill(_)
+            | Op::PixLd(_)
+            | Op::S2R(_)
+            | Op::Nop(_)
+            | Op::Vote(_) => false,
+
+            // Virtual ops
+            Op::Undef(_)
+            | Op::SrcBar(_)
+            | Op::PhiSrcs(_)
+            | Op::PhiDsts(_)
+            | Op::Copy(_)
+            | Op::Pin(_)
+            | Op::Unpin(_)
+            | Op::Swap(_)
+            | Op::ParCopy(_)
+            | Op::RegOut(_)
+            | Op::Annotate(_) => true,
+        }
+    }
+
     pub fn needs_yield(&self) -> bool {
         matches!(&self.op, Op::Bar(_) | Op::BSync(_))
     }
@@ -7239,6 +7374,12 @@ pub struct ShaderInfo {
     pub num_gprs: u8,
     pub num_control_barriers: u8,
     pub num_instrs: u32,
+    pub num_static_cycles: u32,
+    pub num_spills_to_mem: u32,
+    pub num_fills_from_mem: u32,
+    pub num_spills_to_reg: u32,
+    pub num_fills_from_reg: u32,
+    pub occupancy_in_warps_per_sm: u32,
     pub slm_size: u32,
     pub max_crs_depth: u32,
     pub uses_global_mem: bool,
@@ -7279,6 +7420,21 @@ pub fn gpr_limit_from_local_size(local_size: &[u16; 3]) -> u32 {
     min(out, 255)
 }
 
+pub fn occupancy_in_warps_per_sm(gprs: u32) -> u32 {
+    fn prev_multiple_of(x: u32, y: u32) -> u32 {
+        (x / y) * y
+    }
+
+    // TODO: Take local_szie and shared mem limit into account for compute
+    let total_regs: u32 = 65536;
+    // GPRs are allocated in multiples of 8
+    let gprs = max(gprs, 1);
+    let gprs = gprs.next_multiple_of(8);
+    let max_warps = prev_multiple_of((total_regs / 32) / gprs, 4);
+    let max_warps = min(max_warps, 48);
+    max_warps
+}
+
 pub struct Shader<'a> {
     pub sm: &'a dyn ShaderModel,
     pub info: ShaderInfo,
@@ -7318,11 +7474,13 @@ impl Shader<'_> {
 
     pub fn gather_info(&mut self) {
         let mut num_instrs = 0;
+        let mut num_static_cycles = 0;
         let mut uses_global_mem = false;
         let mut writes_global_mem = false;
 
         self.for_each_instr(&mut |instr| {
             num_instrs += 1;
+            num_static_cycles += instr.deps.delay as u32;
 
             if !uses_global_mem {
                 uses_global_mem = instr.uses_global_mem();
@@ -7334,8 +7492,13 @@ impl Shader<'_> {
         });
 
         self.info.num_instrs = num_instrs;
+        self.info.num_static_cycles = num_static_cycles;
         self.info.uses_global_mem = uses_global_mem;
         self.info.writes_global_mem = writes_global_mem;
+
+        self.info.occupancy_in_warps_per_sm = occupancy_in_warps_per_sm(
+            self.info.num_gprs as u32 + self.sm.hw_reserved_gprs(),
+        );
     }
 }
 
diff --git a/src/nouveau/compiler/nak/lib.rs b/src/nouveau/compiler/nak/lib.rs
index 178067d56ba3cba3b9271df6d2ab55c56cc3e702..7650e0cbfc9ae7076be8a9f7c8abd821c52059ce 100644
--- a/src/nouveau/compiler/nak/lib.rs
+++ b/src/nouveau/compiler/nak/lib.rs
@@ -15,6 +15,9 @@ mod opt_bar_prop;
 mod opt_copy_prop;
 mod opt_crs;
 mod opt_dce;
+mod opt_instr_sched_common;
+mod opt_instr_sched_postpass;
+mod opt_instr_sched_prepass;
 mod opt_jump_thread;
 mod opt_lop;
 mod opt_out;
@@ -22,6 +25,7 @@ mod opt_prmt;
 mod opt_uniform_instrs;
 mod qmd;
 mod repair_ssa;
+mod sched_common;
 mod sm50;
 mod sm70;
 mod sph;
diff --git a/src/nouveau/compiler/nak/liveness.rs b/src/nouveau/compiler/nak/liveness.rs
index 349f2f84d2c86f72e6f202cdcba5451474197869..7132d731b62f1936637e6abbb3016187e51be662 100644
--- a/src/nouveau/compiler/nak/liveness.rs
+++ b/src/nouveau/compiler/nak/liveness.rs
@@ -8,7 +8,7 @@ use std::cell::RefCell;
 use std::cmp::{max, Ord, Ordering};
 use std::collections::{hash_set, HashMap, HashSet};
 
-#[derive(Clone)]
+#[derive(Clone, Default)]
 pub struct LiveSet {
     live: PerRegFile<u32>,
     set: HashSet<SSAValue>,
@@ -16,10 +16,12 @@ pub struct LiveSet {
 
 impl LiveSet {
     pub fn new() -> LiveSet {
-        LiveSet {
-            live: Default::default(),
-            set: HashSet::new(),
-        }
+        Default::default()
+    }
+
+    pub fn clear(&mut self) {
+        self.live = Default::default();
+        self.set.clear();
     }
 
     pub fn contains(&self, ssa: &SSAValue) -> bool {
diff --git a/src/nouveau/compiler/nak/opt_instr_sched_common.rs b/src/nouveau/compiler/nak/opt_instr_sched_common.rs
new file mode 100644
index 0000000000000000000000000000000000000000..cf637f4c841e22ff0c549dec4aaf26727344646b
--- /dev/null
+++ b/src/nouveau/compiler/nak/opt_instr_sched_common.rs
@@ -0,0 +1,389 @@
+// Copyright © 2024 Valve Corporation
+// SPDX-License-Identifier: MIT
+
+use crate::ir::*;
+use std::cmp::max;
+use std::cmp::Reverse;
+
+pub mod graph {
+    #[derive(Clone)]
+    pub struct Edge<EdgeLabel> {
+        pub label: EdgeLabel,
+        pub head_idx: usize,
+    }
+
+    #[derive(Clone)]
+    pub struct Node<NodeLabel, EdgeLabel> {
+        pub label: NodeLabel,
+        pub outgoing_edges: Vec<Edge<EdgeLabel>>,
+    }
+
+    #[derive(Clone)]
+    pub struct Graph<NodeLabel, EdgeLabel> {
+        pub nodes: Vec<Node<NodeLabel, EdgeLabel>>,
+    }
+
+    impl<NodeLabel, EdgeLabel> Graph<NodeLabel, EdgeLabel> {
+        pub fn new(node_labels: impl Iterator<Item = NodeLabel>) -> Self {
+            let nodes = node_labels
+                .map(|label| Node {
+                    label,
+                    outgoing_edges: Vec::new(),
+                })
+                .collect();
+
+            Graph { nodes }
+        }
+
+        pub fn add_edge(
+            &mut self,
+            tail_idx: usize,
+            head_idx: usize,
+            label: EdgeLabel,
+        ) {
+            assert!(head_idx < self.nodes.len());
+            self.nodes[tail_idx]
+                .outgoing_edges
+                .push(Edge { label, head_idx });
+        }
+
+        pub fn reverse(&mut self) {
+            let old_edges: Vec<_> = self
+                .nodes
+                .iter_mut()
+                .map(|node| std::mem::take(&mut node.outgoing_edges))
+                .collect();
+
+            for (old_tail_idx, old_outgoing_edges) in
+                old_edges.into_iter().enumerate()
+            {
+                for Edge {
+                    label,
+                    head_idx: old_head_idx,
+                } in old_outgoing_edges.into_iter()
+                {
+                    self.add_edge(old_head_idx, old_tail_idx, label);
+                }
+            }
+        }
+    }
+}
+
+#[derive(Eq, PartialEq)]
+pub enum SideEffect {
+    /// No side effect (ALU-like)
+    None,
+
+    /// Instruction reads or writes memory
+    ///
+    /// This will be serialized with respect to other
+    /// SideEffect::Memory instructions
+    Memory,
+
+    /// This instcuction is a full code motion barrier
+    ///
+    /// No other instruction will be re-ordered with respect to this one
+    Barrier,
+}
+
+pub fn side_effect_type(op: &Op) -> SideEffect {
+    match op {
+        // Float ALU
+        Op::F2FP(_)
+        | Op::FAdd(_)
+        | Op::FFma(_)
+        | Op::FMnMx(_)
+        | Op::FMul(_)
+        | Op::FSet(_)
+        | Op::FSetP(_)
+        | Op::HAdd2(_)
+        | Op::HFma2(_)
+        | Op::HMul2(_)
+        | Op::HSet2(_)
+        | Op::HSetP2(_)
+        | Op::HMnMx2(_)
+        | Op::FSwzAdd(_) => SideEffect::None,
+
+        // Multi-function unit
+        Op::Rro(_) | Op::MuFu(_) => SideEffect::None,
+
+        // Double-precision float ALU
+        Op::DAdd(_)
+        | Op::DFma(_)
+        | Op::DMnMx(_)
+        | Op::DMul(_)
+        | Op::DSetP(_) => SideEffect::None,
+
+        // Integer ALU
+        Op::BRev(_)
+        | Op::Flo(_)
+        | Op::PopC(_)
+        | Op::IMad(_)
+        | Op::IMul(_)
+        | Op::BMsk(_)
+        | Op::IAbs(_)
+        | Op::IAdd2(_)
+        | Op::IAdd2X(_)
+        | Op::IAdd3(_)
+        | Op::IAdd3X(_)
+        | Op::IDp4(_)
+        | Op::IMad64(_)
+        | Op::IMnMx(_)
+        | Op::ISetP(_)
+        | Op::Lop2(_)
+        | Op::Lop3(_)
+        | Op::Shf(_)
+        | Op::Shl(_)
+        | Op::Shr(_)
+        | Op::Bfe(_) => SideEffect::None,
+
+        // Conversions
+        Op::F2F(_) | Op::F2I(_) | Op::I2F(_) | Op::I2I(_) | Op::FRnd(_) => {
+            SideEffect::None
+        }
+
+        // Move ops
+        Op::Mov(_) | Op::Prmt(_) | Op::Sel(_) => SideEffect::None,
+        Op::Shfl(_) => SideEffect::None,
+
+        // Predicate ops
+        Op::PLop3(_) | Op::PSetP(_) => SideEffect::None,
+
+        // Uniform ops
+        Op::R2UR(_) => SideEffect::None,
+
+        // Texture ops
+        Op::Tex(_)
+        | Op::Tld(_)
+        | Op::Tld4(_)
+        | Op::Tmml(_)
+        | Op::Txd(_)
+        | Op::Txq(_) => SideEffect::Memory,
+
+        // Surface ops
+        Op::SuLd(_) | Op::SuSt(_) | Op::SuAtom(_) => SideEffect::Memory,
+
+        // Memory ops
+        Op::Ipa(_) | Op::Ldc(_) => SideEffect::None,
+        Op::Ld(_)
+        | Op::St(_)
+        | Op::Atom(_)
+        | Op::AL2P(_)
+        | Op::ALd(_)
+        | Op::ASt(_)
+        | Op::CCtl(_)
+        | Op::LdTram(_)
+        | Op::MemBar(_) => SideEffect::Memory,
+
+        // Control-flow ops
+        Op::BClear(_)
+        | Op::Break(_)
+        | Op::BSSy(_)
+        | Op::BSync(_)
+        | Op::SSy(_)
+        | Op::Sync(_)
+        | Op::Brk(_)
+        | Op::PBk(_)
+        | Op::Cont(_)
+        | Op::PCnt(_)
+        | Op::Bra(_)
+        | Op::Exit(_)
+        | Op::WarpSync(_) => SideEffect::Barrier,
+
+        // We don't model the barrier register yet, so serialize these
+        Op::BMov(_) => SideEffect::Memory,
+
+        // Geometry ops
+        Op::Out(_) | Op::OutFinal(_) => SideEffect::Barrier,
+
+        // Miscellaneous ops
+        Op::Bar(_)
+        | Op::CS2R(_)
+        | Op::Isberd(_)
+        | Op::Kill(_)
+        | Op::PixLd(_)
+        | Op::S2R(_) => SideEffect::Barrier,
+        Op::Nop(_) | Op::Vote(_) => SideEffect::None,
+
+        // Virtual ops
+        Op::Annotate(_)
+        | Op::ParCopy(_)
+        | Op::Swap(_)
+        | Op::Copy(_)
+        | Op::Undef(_) => SideEffect::None,
+
+        Op::SrcBar(_)
+        | Op::Pin(_)
+        | Op::Unpin(_)
+        | Op::PhiSrcs(_)
+        | Op::PhiDsts(_)
+        | Op::RegOut(_) => SideEffect::Barrier,
+    }
+}
+
+/// Try to guess how many cycles a variable latency instruction will take
+///
+/// These values are based on the cycle estimates from "Dissecting the NVidia
+/// Turing T4 GPU via Microbenchmarking" https://arxiv.org/pdf/1903.07486
+/// Memory instructions were copied from L1 data cache latencies.
+/// For instructions not mentioned in the paper, I made up numbers.
+/// This could probably be improved.
+pub fn estimate_variable_latency(sm: u8, op: &Op) -> u32 {
+    match op {
+        // Multi-function unit
+        Op::Rro(_) | Op::MuFu(_) => 15,
+
+        // Double-precision float ALU
+        Op::DFma(_) | Op::DSetP(_) => 54,
+        Op::DAdd(_) | Op::DMnMx(_) | Op::DMul(_) => 48,
+
+        // Integer ALU
+        Op::BRev(_) | Op::Flo(_) | Op::PopC(_) => 15,
+        Op::IMad(_) | Op::IMul(_) => {
+            assert!(sm < 70);
+            86
+        }
+
+        // Conversions
+        Op::F2F(_) | Op::F2I(_) | Op::I2F(_) | Op::I2I(_) | Op::FRnd(_) => 15,
+
+        // Move ops
+        Op::Shfl(_) => 15,
+
+        // Uniform ops
+        Op::R2UR(_) => 15,
+
+        // Texture ops
+        Op::Tex(_)
+        | Op::Tld(_)
+        | Op::Tld4(_)
+        | Op::Tmml(_)
+        | Op::Txd(_)
+        | Op::Txq(_) => 32,
+
+        // Surface ops
+        Op::SuLd(_) | Op::SuSt(_) | Op::SuAtom(_) => 32,
+
+        // Memory ops
+        Op::Ldc(_) => 4,
+
+        Op::Ld(_)
+        | Op::St(_)
+        | Op::Atom(_)
+        | Op::AL2P(_)
+        | Op::ALd(_)
+        | Op::ASt(_)
+        | Op::Ipa(_)
+        | Op::CCtl(_)
+        | Op::LdTram(_)
+        | Op::MemBar(_) => 32,
+
+        _ => panic!("Unknown variable latency op {op}"),
+    }
+}
+
+#[derive(Default, Clone)]
+pub struct NodeLabel {
+    pub cycles_to_end: u32,
+    pub num_uses: u32,
+    pub ready_cycle: u32,
+}
+
+pub struct EdgeLabel {
+    pub latency: u32,
+}
+
+pub type DepGraph = graph::Graph<NodeLabel, EdgeLabel>;
+
+pub fn calc_statistics(g: &mut DepGraph) -> Vec<usize> {
+    let mut initial_ready_list = Vec::new();
+    for i in (0..g.nodes.len()).rev() {
+        let node = &g.nodes[i];
+        let mut max_delay = 0;
+        for edge in &node.outgoing_edges {
+            assert!(edge.head_idx > i);
+            max_delay = max(
+                max_delay,
+                g.nodes[edge.head_idx].label.cycles_to_end + edge.label.latency,
+            );
+        }
+        let node = &mut g.nodes[i];
+        node.label.cycles_to_end = max_delay;
+        node.label.num_uses = node.outgoing_edges.len().try_into().unwrap();
+        if node.label.num_uses == 0 {
+            initial_ready_list.push(i);
+        }
+    }
+    return initial_ready_list;
+}
+
+#[derive(Clone, PartialEq, Eq, PartialOrd, Ord)]
+pub struct ReadyInstr {
+    cycles_to_end: u32,
+
+    // We use the original instruction order as a final tie-breaker, the idea
+    // being that the original schedule is often not too bad. Since we're
+    // iterating in reverse order, that means scheduling the largest instruciton
+    // index first.
+    pub index: usize,
+}
+
+impl ReadyInstr {
+    pub fn new<E>(g: &graph::Graph<NodeLabel, E>, i: usize) -> Self {
+        ReadyInstr {
+            cycles_to_end: g.nodes[i].label.cycles_to_end,
+            index: i,
+        }
+    }
+}
+
+#[derive(Clone, PartialEq, Eq, PartialOrd, Ord)]
+pub struct FutureReadyInstr {
+    pub ready_cycle: Reverse<u32>,
+    pub index: usize,
+}
+
+impl FutureReadyInstr {
+    pub fn new<E>(g: &graph::Graph<NodeLabel, E>, i: usize) -> Self {
+        FutureReadyInstr {
+            ready_cycle: Reverse(g.nodes[i].label.ready_cycle),
+            index: i,
+        }
+    }
+}
+
+#[allow(dead_code)]
+pub fn save_graphviz(
+    instrs: &[Box<Instr>],
+    g: &DepGraph,
+) -> std::io::Result<()> {
+    // dot /tmp/instr_dep_graph.dot -Tsvg > /tmp/instr_dep_graph.svg
+
+    use std::fs::File;
+    use std::io::{BufWriter, Write};
+
+    let file = File::create("/tmp/instr_dep_graph.dot")?;
+    let mut w = BufWriter::new(file);
+
+    writeln!(w, "digraph {{")?;
+    for (i, instr) in instrs.iter().enumerate() {
+        let l = &g.nodes[i].label;
+        writeln!(
+            w,
+            "    {i} [label=\"{}\\n{}, {}\"];",
+            instr, l.cycles_to_end, l.num_uses
+        )?;
+    }
+    for (i, node) in g.nodes.iter().enumerate() {
+        for j in &node.outgoing_edges {
+            writeln!(
+                w,
+                "    {i} -> {} [label=\"{}\"];",
+                j.head_idx, j.label.latency
+            )?;
+        }
+    }
+    writeln!(w, "}}")?;
+    w.flush()?;
+    Ok(())
+}
diff --git a/src/nouveau/compiler/nak/opt_instr_sched_postpass.rs b/src/nouveau/compiler/nak/opt_instr_sched_postpass.rs
new file mode 100644
index 0000000000000000000000000000000000000000..51723eb42fb19d69f8ca28964795d725b6ca11a8
--- /dev/null
+++ b/src/nouveau/compiler/nak/opt_instr_sched_postpass.rs
@@ -0,0 +1,244 @@
+// Copyright © 2024 Valve Corporation
+// SPDX-License-Identifier: MIT
+
+use crate::ir::*;
+use crate::opt_instr_sched_common::*;
+use crate::sched_common::{
+    paw_latency, raw_latency, war_latency, waw_latency, RegTracker,
+};
+use std::cmp::max;
+use std::cmp::Reverse;
+use std::collections::BinaryHeap;
+
+struct RegUse<T: Clone> {
+    reads: Vec<T>,
+    write: Option<T>,
+}
+
+impl<T: Clone> RegUse<T> {
+    pub fn new() -> Self {
+        RegUse {
+            reads: Vec::new(),
+            write: None,
+        }
+    }
+
+    pub fn add_read(&mut self, dep: T) {
+        self.reads.push(dep);
+    }
+
+    pub fn set_write(&mut self, dep: T) {
+        self.write = Some(dep);
+        self.reads.clear();
+    }
+}
+
+fn generate_dep_graph(
+    sm: &dyn ShaderModel,
+    instrs: &Vec<Box<Instr>>,
+) -> DepGraph {
+    let mut g = DepGraph::new((0..instrs.len()).map(|_| Default::default()));
+
+    // Maps registers to RegUse<ip, src_dst_idx>.  Predicates are
+    // represented by  src_idx = usize::MAX.
+    let mut uses: RegTracker<RegUse<(usize, usize)>> =
+        RegTracker::new_with(&|| RegUse::new());
+
+    let mut last_memory_op = None;
+
+    for ip in (0..instrs.len()).rev() {
+        let instr = &instrs[ip];
+
+        if side_effect_type(&instr.op) == SideEffect::Memory {
+            if let Some(mem_ip) = last_memory_op {
+                g.add_edge(ip, mem_ip, EdgeLabel { latency: 0 });
+            }
+            last_memory_op = Some(ip);
+        }
+
+        uses.for_each_instr_dst_mut(instr, |i, u| {
+            if let Some((w_ip, w_dst_idx)) = u.write {
+                let latency = waw_latency(
+                    sm.sm(),
+                    &instr.op,
+                    i,
+                    &instrs[w_ip].op,
+                    w_dst_idx,
+                );
+                g.add_edge(ip, w_ip, EdgeLabel { latency });
+            }
+
+            for &(r_ip, r_src_idx) in &u.reads {
+                let mut latency = if r_src_idx == usize::MAX {
+                    paw_latency(sm.sm(), &instr.op, i)
+                } else {
+                    raw_latency(
+                        sm.sm(),
+                        &instr.op,
+                        i,
+                        &instrs[r_ip].op,
+                        r_src_idx,
+                    )
+                };
+                if !instr.has_fixed_latency(sm.sm()) {
+                    latency = max(
+                        latency,
+                        estimate_variable_latency(sm.sm(), &instr.op),
+                    );
+                }
+                g.add_edge(ip, r_ip, EdgeLabel { latency });
+            }
+        });
+        uses.for_each_instr_src_mut(instr, |i, u| {
+            if let Some((w_ip, w_dst_idx)) = u.write {
+                let latency = war_latency(
+                    sm.sm(),
+                    &instr.op,
+                    i,
+                    &instrs[w_ip].op,
+                    w_dst_idx,
+                );
+                g.add_edge(ip, w_ip, EdgeLabel { latency });
+            }
+        });
+
+        uses.for_each_instr_pred_mut(instr, |c| {
+            c.add_read((ip, usize::MAX));
+        });
+        uses.for_each_instr_src_mut(instr, |i, c| {
+            c.add_read((ip, i));
+        });
+        uses.for_each_instr_dst_mut(instr, |i, c| {
+            c.set_write((ip, i));
+        });
+    }
+
+    g
+}
+
+fn generate_order(g: &mut DepGraph, init_ready_list: Vec<usize>) -> Vec<usize> {
+    let mut ready_instrs: BinaryHeap<ReadyInstr> = init_ready_list
+        .into_iter()
+        .map(|i| ReadyInstr::new(g, i))
+        .collect();
+    let mut future_ready_instrs = BinaryHeap::new();
+
+    let mut current_cycle = 0;
+    let mut instr_order = Vec::with_capacity(g.nodes.len());
+    loop {
+        // Move ready instructions to the ready list
+        loop {
+            match future_ready_instrs.peek() {
+                None => break,
+                Some(FutureReadyInstr {
+                    ready_cycle: std::cmp::Reverse(ready_cycle),
+                    index,
+                }) => {
+                    if current_cycle >= *ready_cycle {
+                        ready_instrs.push(ReadyInstr::new(g, *index));
+                        future_ready_instrs.pop();
+                    } else {
+                        break;
+                    }
+                }
+            }
+        }
+
+        // Pick a ready instruction
+        let next_idx = match ready_instrs.pop() {
+            None => {
+                match future_ready_instrs.peek() {
+                    None => break, // Both lists are empty. We're done!
+                    Some(&FutureReadyInstr {
+                        ready_cycle: Reverse(ready_cycle),
+                        ..
+                    }) => {
+                        // Fast-forward time to when the next instr is ready
+                        assert!(ready_cycle > current_cycle);
+                        current_cycle = ready_cycle;
+                        continue;
+                    }
+                }
+            }
+            Some(ReadyInstr { index, .. }) => index,
+        };
+
+        // Schedule the instuction
+        let outgoing_edges =
+            std::mem::take(&mut g.nodes[next_idx].outgoing_edges);
+        for edge in outgoing_edges.into_iter() {
+            let dep_instr = &mut g.nodes[edge.head_idx].label;
+            dep_instr.ready_cycle =
+                max(dep_instr.ready_cycle, current_cycle + edge.label.latency);
+            dep_instr.num_uses -= 1;
+            if dep_instr.num_uses <= 0 {
+                future_ready_instrs
+                    .push(FutureReadyInstr::new(g, edge.head_idx));
+            }
+        }
+
+        instr_order.push(next_idx);
+        current_cycle += 1;
+    }
+    return instr_order;
+}
+
+fn sched_buffer(
+    sm: &dyn ShaderModel,
+    instrs: Vec<Box<Instr>>,
+) -> impl Iterator<Item = Box<Instr>> {
+    let mut g = generate_dep_graph(sm, &instrs);
+    let init_ready_list = calc_statistics(&mut g);
+    // save_graphviz(&instrs, &g).unwrap();
+    g.reverse();
+    let new_order = generate_order(&mut g, init_ready_list);
+
+    // Apply the new instruction order
+    let mut instrs: Vec<Option<Box<Instr>>> =
+        instrs.into_iter().map(|instr| Some(instr)).collect();
+    new_order.into_iter().rev().map(move |i| {
+        std::mem::take(&mut instrs[i]).expect("Instruction scheduled twice")
+    })
+}
+
+impl Function {
+    pub fn opt_instr_sched_postpass(&mut self, sm: &dyn ShaderModel) {
+        for block in &mut self.blocks {
+            let orig_instr_count = block.instrs.len();
+            let mut reorder_buffer = Vec::new();
+            for instr in std::mem::take(&mut block.instrs) {
+                match side_effect_type(&instr.op) {
+                    SideEffect::None | SideEffect::Memory => {
+                        reorder_buffer.push(instr);
+                    }
+                    SideEffect::Barrier => {
+                        if !reorder_buffer.is_empty() {
+                            block.instrs.extend(sched_buffer(
+                                sm,
+                                std::mem::take(&mut reorder_buffer),
+                            ));
+                        }
+                        block.instrs.push(instr);
+                    }
+                }
+            }
+            if !reorder_buffer.is_empty() {
+                block.instrs.extend(sched_buffer(sm, reorder_buffer));
+            }
+            assert_eq!(orig_instr_count, block.instrs.len());
+        }
+    }
+}
+
+impl Shader<'_> {
+    /// Post-RA instruction scheduling
+    ///
+    /// Uses the popular latency-weighted-depth heuristic.
+    /// See eg. Cooper & Torczon's "Engineering A Compiler", 3rd ed.
+    /// Chapter 12.3 "Local scheduling"
+    pub fn opt_instr_sched_postpass(&mut self) {
+        for f in &mut self.functions {
+            f.opt_instr_sched_postpass(self.sm);
+        }
+    }
+}
diff --git a/src/nouveau/compiler/nak/opt_instr_sched_prepass.rs b/src/nouveau/compiler/nak/opt_instr_sched_prepass.rs
new file mode 100644
index 0000000000000000000000000000000000000000..e478d2cdb4b24062fb32e53c852312a43e02fbb6
--- /dev/null
+++ b/src/nouveau/compiler/nak/opt_instr_sched_prepass.rs
@@ -0,0 +1,977 @@
+use crate::ir::*;
+use crate::liveness::{BlockLiveness, LiveSet, Liveness, SimpleLiveness};
+use crate::opt_instr_sched_common::{
+    calc_statistics, estimate_variable_latency, side_effect_type, DepGraph,
+    EdgeLabel, FutureReadyInstr, ReadyInstr, SideEffect,
+};
+use crate::sched_common::{paw_latency, raw_latency};
+use std::cmp::Reverse;
+use std::cmp::{max, min};
+use std::collections::BTreeSet;
+use std::collections::HashMap;
+
+// This is the maximum number of reserved gprs - (TODO: Only reserve 1 if we
+// don't need 2)
+const SW_RESERVED_GPRS: i32 = 2;
+const SW_RESERVED_GPRS_SPILL: i32 = 2;
+
+/// Target number of free GPRs. This is used for the threshold to switch to
+/// scheduling for register pressure
+const TARGET_FREE: i32 = 4;
+
+/// Typically using an extra register is free... until you hit a threshold where
+/// one more register causes occupancy to plummet. This function figures out how
+/// many GPRs you can use without costing occupancy, assuming you always need at
+/// least `x` GPRs.
+fn next_occupancy_cliff(x: u32) -> u32 {
+    fn prev_multiple_of(x: u32, y: u32) -> u32 {
+        (x / y) * y
+    }
+
+    let total_regs: u32 = 65536;
+    let threads = occupancy_in_warps_per_sm(x) * 32;
+
+    // This function doesn't actually model the maximum number of registers
+    // correctly - callers need to worry about that separately. We do,
+    // however, want to avoid a divide by zero.
+    let threads = max(threads, 1);
+
+    prev_multiple_of(total_regs / threads, 8)
+}
+
+#[cfg(test)]
+#[test]
+fn test_next_occupancy_cliff() {
+    for x in 0..255 {
+        let y = next_occupancy_cliff(x);
+        assert!(y >= x);
+        assert_eq!(occupancy_in_warps_per_sm(x), occupancy_in_warps_per_sm(y));
+        assert!(
+            occupancy_in_warps_per_sm(y) > occupancy_in_warps_per_sm(y + 1)
+        );
+    }
+}
+
+fn next_occupancy_cliff_with_reserved(gprs: i32, reserved: i32) -> i32 {
+    i32::try_from(next_occupancy_cliff((gprs + reserved).try_into().unwrap()))
+        .unwrap()
+        - reserved
+}
+
+fn generate_dep_graph(sm: &dyn ShaderModel, instrs: &[Box<Instr>]) -> DepGraph {
+    let mut g = DepGraph::new((0..instrs.len()).map(|_| Default::default()));
+
+    let mut defs = HashMap::<SSAValue, (usize, usize)>::new();
+
+    let mut last_memory_op = None;
+
+    for ip in 0..instrs.len() {
+        let instr = &instrs[ip];
+
+        if side_effect_type(&instr.op) == SideEffect::Memory {
+            if let Some(mem_ip) = last_memory_op {
+                g.add_edge(mem_ip, ip, EdgeLabel { latency: 0 });
+            }
+            last_memory_op = Some(ip);
+        }
+
+        for (i, src) in instr.srcs().iter().enumerate() {
+            for ssa in src.src_ref.iter_ssa() {
+                if let Some(&(def_ip, def_idx)) = defs.get(ssa) {
+                    let def_instr = &instrs[def_ip];
+                    let mut latency = raw_latency(
+                        sm.sm(),
+                        &def_instr.op,
+                        def_idx,
+                        &instr.op,
+                        i,
+                    );
+
+                    if !def_instr.is_virtual()
+                        && !def_instr.has_fixed_latency(sm.sm())
+                    {
+                        latency = max(
+                            latency,
+                            estimate_variable_latency(sm.sm(), &def_instr.op),
+                        );
+                    }
+
+                    g.add_edge(def_ip, ip, EdgeLabel { latency });
+                }
+            }
+        }
+
+        if let PredRef::SSA(ssa) = &instr.pred.pred_ref {
+            if let Some(&(def_ip, def_idx)) = defs.get(ssa) {
+                let def_instr = &instrs[def_ip];
+                let mut latency = paw_latency(sm.sm(), &def_instr.op, def_idx);
+
+                if !def_instr.has_fixed_latency(sm.sm()) {
+                    latency = max(
+                        latency,
+                        estimate_variable_latency(sm.sm(), &def_instr.op),
+                    );
+                }
+
+                g.add_edge(def_ip, ip, EdgeLabel { latency });
+            }
+        }
+
+        for (i, dst) in instr.dsts().iter().enumerate() {
+            for &ssa in dst.iter_ssa() {
+                defs.insert(ssa, (ip, i));
+            }
+        }
+    }
+
+    g
+}
+
+mod net_live {
+    use crate::ir::*;
+    use crate::liveness::LiveSet;
+    use std::collections::HashMap;
+    use std::ops::Index;
+
+    /// The net change in live values, from the end of an instruction to a
+    /// specific point during the instruction's execution
+    pub(super) struct InstrCount {
+        /// The net change in live values across the whole instruction
+        pub net: PerRegFile<i8>,
+
+        /// peak1 is at the end of the instruction, where any immediately-killed
+        /// defs are live
+        pub peak1: PerRegFile<i8>,
+
+        /// peak2 is just before sources are read, and after vector defs are live
+        pub peak2: PerRegFile<i8>,
+    }
+
+    /// For each instruction, keep track of a "net live" value, which is how
+    /// much the size of the live values set will change if we chedule a given
+    /// instruction next. This is tracked per-register-file.
+    ///
+    /// Assumes that we are iterating over instructions in reverse order
+    pub(super) struct NetLive {
+        counts: Vec<InstrCount>,
+        ssa_to_instr: HashMap<SSAValue, Vec<usize>>,
+    }
+
+    impl NetLive {
+        pub(super) fn new(instrs: &[Box<Instr>], live_out: &LiveSet) -> Self {
+            let mut use_set = LiveSet::new();
+            let mut ssa_to_instr = HashMap::new();
+
+            let mut counts: Vec<InstrCount> = instrs
+                .iter()
+                .enumerate()
+                .map(|(instr_idx, instr)| {
+                    use_set.clear();
+                    for src in instr.srcs() {
+                        for ssa in src.iter_ssa() {
+                            if !live_out.contains(ssa) {
+                                if use_set.insert(*ssa) {
+                                    ssa_to_instr
+                                        .entry(*ssa)
+                                        .or_insert_with(Vec::new)
+                                        .push(instr_idx);
+                                }
+                            }
+                        }
+                    }
+
+                    let net = PerRegFile::new_with(|f| {
+                        use_set.count(f).try_into().unwrap()
+                    });
+                    InstrCount {
+                        net: net,
+                        peak1: Default::default(),
+                        peak2: net,
+                    }
+                })
+                .collect();
+
+            for (instr_idx, instr) in instrs.iter().enumerate() {
+                for dst in instr.dsts() {
+                    let is_vector = dst.iter_ssa().len() > 1;
+                    let count = &mut counts[instr_idx];
+
+                    for &ssa in dst.iter_ssa() {
+                        if ssa_to_instr.contains_key(&ssa)
+                            || live_out.contains(&ssa)
+                        {
+                            count.net[ssa.file()] -= 1;
+                        } else {
+                            count.peak1[ssa.file()] += 1;
+                            count.peak2[ssa.file()] += 1;
+                        }
+
+                        if !is_vector {
+                            count.peak2[ssa.file()] -= 1;
+                        }
+                    }
+                }
+            }
+
+            NetLive {
+                counts,
+                ssa_to_instr,
+            }
+        }
+
+        pub(super) fn remove(&mut self, ssa: SSAValue) -> bool {
+            match self.ssa_to_instr.remove(&ssa) {
+                Some(instr_idxs) => {
+                    assert!(!instr_idxs.is_empty());
+                    let file = ssa.file();
+                    for i in instr_idxs {
+                        self.counts[i].net[file] -= 1;
+                        self.counts[i].peak2[file] -= 1;
+                    }
+                    true
+                }
+                None => false,
+            }
+        }
+    }
+
+    impl Index<usize> for NetLive {
+        type Output = InstrCount;
+
+        fn index(&self, index: usize) -> &Self::Output {
+            &self.counts[index]
+        }
+    }
+}
+
+use net_live::NetLive;
+
+/// The third element of each tuple is a weight meant to approximate the cost of
+/// spilling a value from the first register file to the second. Right now, the
+/// values are meant to approximate the cost of a spill + fill, in cycles
+const SPILL_FILES: [(RegFile, RegFile, i32); 5] = [
+    (RegFile::Bar, RegFile::GPR, 6 + 6),
+    (RegFile::Pred, RegFile::GPR, 12 + 6),
+    (RegFile::UPred, RegFile::UGPR, 12 + 6),
+    (RegFile::UGPR, RegFile::GPR, 15 + 6),
+    (RegFile::GPR, RegFile::Mem, 32 + 32),
+];
+
+/// Models how many gprs will be used after spilling other register files
+fn calc_used_gprs(mut p: PerRegFile<i32>, max_regs: PerRegFile<i32>) -> i32 {
+    for (src, dest, _) in SPILL_FILES {
+        if p[src] > max_regs[src] {
+            p[dest] += p[src] - max_regs[src];
+        }
+    }
+
+    p[RegFile::GPR]
+}
+
+fn calc_score_part(
+    mut p: PerRegFile<i32>,
+    max_regs: PerRegFile<i32>,
+) -> (i32, i32) {
+    // We separate "badness" and "goodness" because we don't want eg. two extra
+    // free predicates to offset the weight of spilling a UGPR - the spill is
+    // always more important than keeping extra registers free
+    let mut badness: i32 = 0;
+    let mut goodness: i32 = 0;
+
+    for (src, dest, weight) in SPILL_FILES {
+        if p[src] > max_regs[src] {
+            let spill_count = p[src] - max_regs[src];
+            p[dest] += spill_count;
+            badness += spill_count * weight;
+        } else {
+            let free_count = max_regs[src] - p[src];
+            goodness += free_count * weight;
+        }
+    }
+    (badness, goodness)
+}
+
+type Score = (bool, Reverse<i32>, i32);
+fn calc_score(
+    net: PerRegFile<i32>,
+    peak1: PerRegFile<i32>,
+    peak2: PerRegFile<i32>,
+    max_regs: PerRegFile<i32>,
+    delay_cycles: u32,
+    thresholds: ScheduleThresholds,
+) -> Score {
+    let peak_gprs = max(
+        calc_used_gprs(peak1, max_regs),
+        calc_used_gprs(peak2, max_regs),
+    );
+    let instruction_usable = peak_gprs <= thresholds.quit_threshold;
+    if !instruction_usable {
+        return (false, Reverse(0), 0);
+    }
+
+    let (mut badness, goodness) = calc_score_part(net, max_regs);
+    badness += i32::try_from(delay_cycles).unwrap();
+
+    (true, Reverse(badness), goodness)
+}
+
+#[derive(Copy, Clone)]
+struct ScheduleThresholds {
+    /// Start scheduling for pressure if we use this many gprs
+    heuristic_threshold: i32,
+
+    /// Give up if we use this many gprs
+    quit_threshold: i32,
+}
+
+struct GenerateOrder<'a> {
+    max_regs: PerRegFile<i32>,
+    net_live: NetLive,
+    live: LiveSet,
+    instrs: &'a [Box<Instr>],
+}
+
+impl<'a> GenerateOrder<'a> {
+    fn new(
+        max_regs: PerRegFile<i32>,
+        instrs: &'a [Box<Instr>],
+        live_out: &LiveSet,
+    ) -> Self {
+        let net_live = NetLive::new(instrs, live_out);
+        let live: LiveSet = live_out.clone();
+
+        GenerateOrder {
+            max_regs,
+            net_live,
+            live,
+            instrs,
+        }
+    }
+
+    fn new_used_regs(&self, net: PerRegFile<i8>) -> PerRegFile<i32> {
+        PerRegFile::new_with(|file| {
+            i32::try_from(self.live.count(file)).unwrap() + (net[file] as i32)
+        })
+    }
+
+    fn current_used_gprs(&self) -> i32 {
+        calc_used_gprs(
+            PerRegFile::new_with(|f| self.live.count(f).try_into().unwrap()),
+            self.max_regs,
+        )
+    }
+
+    fn new_used_gprs_net(&self, instr_index: usize) -> i32 {
+        calc_used_gprs(
+            self.new_used_regs(self.net_live[instr_index].net),
+            self.max_regs,
+        )
+    }
+
+    fn new_used_gprs_peak1(&self, instr_index: usize) -> i32 {
+        calc_used_gprs(
+            self.new_used_regs(self.net_live[instr_index].peak1),
+            self.max_regs,
+        )
+    }
+
+    fn new_used_gprs_peak2(&self, instr_index: usize) -> i32 {
+        calc_used_gprs(
+            self.new_used_regs(self.net_live[instr_index].peak2),
+            self.max_regs,
+        )
+    }
+
+    fn new_score(
+        &self,
+        instr_index: usize,
+        delay_cycles: u32,
+        thresholds: ScheduleThresholds,
+    ) -> Score {
+        calc_score(
+            self.new_used_regs(self.net_live[instr_index].net),
+            self.new_used_regs(self.net_live[instr_index].peak1),
+            self.new_used_regs(self.net_live[instr_index].peak2),
+            self.max_regs,
+            delay_cycles,
+            thresholds,
+        )
+    }
+
+    fn generate_order(
+        mut self,
+        g: &mut DepGraph,
+        init_ready_list: Vec<usize>,
+        thresholds: ScheduleThresholds,
+    ) -> Option<(Vec<usize>, PerRegFile<i32>)> {
+        let mut ready_instrs: BTreeSet<ReadyInstr> = init_ready_list
+            .into_iter()
+            .map(|i| ReadyInstr::new(g, i))
+            .collect();
+        let mut future_ready_instrs = BTreeSet::new();
+
+        let mut current_cycle = 0;
+        let mut instr_order = Vec::with_capacity(g.nodes.len());
+        loop {
+            let used_gprs = self.current_used_gprs();
+
+            // Move ready instructions to the ready list
+            loop {
+                match future_ready_instrs.last() {
+                    None => break,
+                    Some(FutureReadyInstr {
+                        ready_cycle: std::cmp::Reverse(ready_cycle),
+                        index,
+                    }) => {
+                        if current_cycle >= *ready_cycle {
+                            ready_instrs.insert(ReadyInstr::new(g, *index));
+                            future_ready_instrs.pop_last();
+                        } else {
+                            break;
+                        }
+                    }
+                }
+            }
+
+            if ready_instrs.is_empty() {
+                match future_ready_instrs.last() {
+                    None => break, // Both lists are empty. We're done!
+                    Some(&FutureReadyInstr {
+                        ready_cycle: Reverse(ready_cycle),
+                        ..
+                    }) => {
+                        // Fast-forward time to when the next instr is ready
+                        assert!(ready_cycle > current_cycle);
+                        current_cycle = ready_cycle;
+                        continue;
+                    }
+                }
+            }
+
+            // Pick an instruction to schedule
+            let next_idx = if used_gprs <= thresholds.heuristic_threshold {
+                let ReadyInstr { index, .. } = ready_instrs.pop_last().unwrap();
+                index
+            } else {
+                let (new_score, ready_instr) = ready_instrs
+                    .iter()
+                    .map(|ready_instr| {
+                        (
+                            self.new_score(ready_instr.index, 0, thresholds),
+                            ready_instr.clone(),
+                        )
+                    })
+                    .max()
+                    .unwrap();
+
+                let better_candidate = future_ready_instrs
+                    .iter()
+                    .filter_map(|future_ready_instr| {
+                        let ready_cycle = future_ready_instr.ready_cycle.0;
+                        let s = self.new_score(
+                            future_ready_instr.index,
+                            ready_cycle - current_cycle,
+                            thresholds,
+                        );
+                        if s > new_score {
+                            Some((s, future_ready_instr.clone()))
+                        } else {
+                            None
+                        }
+                    })
+                    .max();
+
+                if let Some((_, future_ready_instr)) = better_candidate {
+                    future_ready_instrs.remove(&future_ready_instr);
+                    let ready_cycle = future_ready_instr.ready_cycle.0;
+                    // Fast-forward time to when this instr is ready
+                    assert!(ready_cycle > current_cycle);
+                    current_cycle = ready_cycle;
+                    future_ready_instr.index
+                } else {
+                    ready_instrs.remove(&ready_instr);
+                    ready_instr.index
+                }
+            };
+
+            // Schedule the instuction
+            let predicted_new_used_gprs_peak = max(
+                self.new_used_gprs_peak1(next_idx),
+                self.new_used_gprs_peak2(next_idx),
+            );
+            let predicted_new_used_gprs_net = self.new_used_gprs_net(next_idx);
+
+            if predicted_new_used_gprs_peak > thresholds.quit_threshold {
+                return None;
+            }
+
+            let outgoing_edges =
+                std::mem::take(&mut g.nodes[next_idx].outgoing_edges);
+            for edge in outgoing_edges.into_iter() {
+                let dep_instr = &mut g.nodes[edge.head_idx].label;
+                dep_instr.ready_cycle = max(
+                    dep_instr.ready_cycle,
+                    current_cycle + edge.label.latency,
+                );
+                dep_instr.num_uses -= 1;
+                if dep_instr.num_uses <= 0 {
+                    future_ready_instrs
+                        .insert(FutureReadyInstr::new(g, edge.head_idx));
+                }
+            }
+
+            // We're walking backwards, so the instr's defs are killed
+            let instr = &self.instrs[next_idx];
+            for dst in instr.dsts() {
+                for ssa in dst.iter_ssa() {
+                    self.live.remove(ssa);
+                }
+            }
+
+            // We're walking backwards, so uses are now live
+            for src in instr.srcs() {
+                for &ssa in src.iter_ssa() {
+                    if self.net_live.remove(ssa) {
+                        self.live.insert(ssa);
+                    } else {
+                        // This branch should only happen if one instruction
+                        // uses the same SSAValue multiple times
+                        debug_assert!(!self.live.insert(ssa));
+                    }
+                }
+            }
+
+            instr_order.push(next_idx);
+            current_cycle += 1;
+
+            debug_assert_eq!(
+                self.current_used_gprs(),
+                predicted_new_used_gprs_net
+            );
+        }
+
+        return Some((
+            instr_order,
+            PerRegFile::new_with(|f| self.live.count(f).try_into().unwrap()),
+        ));
+    }
+}
+
+struct InstructionOrder {
+    order: Vec<usize>,
+}
+
+impl InstructionOrder {
+    fn apply<'a>(
+        &'a self,
+        instrs: Vec<Box<Instr>>,
+    ) -> impl 'a + Iterator<Item = Box<Instr>> {
+        assert_eq!(self.order.len(), instrs.len());
+
+        let mut instrs: Vec<Option<Box<Instr>>> =
+            instrs.into_iter().map(|instr| Some(instr)).collect();
+
+        self.order.iter().map(move |&i| {
+            std::mem::take(&mut instrs[i]).expect("Instruction scheduled twice")
+        })
+    }
+}
+
+fn sched_buffer(
+    sm: &dyn ShaderModel,
+    max_regs: PerRegFile<i32>,
+    instrs: &[Box<Instr>],
+    live_in_count: PerRegFile<u32>,
+    live_out: &LiveSet,
+    thresholds: ScheduleThresholds,
+) -> Option<InstructionOrder> {
+    let mut g = generate_dep_graph(sm, instrs);
+
+    let init_ready_list = calc_statistics(&mut g);
+
+    // use crate::opt_instr_sched_common::save_graphviz;
+    // save_graphviz(instrs, &g).unwrap();
+    g.reverse();
+
+    let (mut new_order, live_in_count2) = GenerateOrder::new(
+        max_regs, instrs, live_out,
+    )
+    .generate_order(&mut g, init_ready_list, thresholds)?;
+
+    // If our accounting is correct, it should match live_in
+    assert_eq!(
+        live_in_count2,
+        PerRegFile::new_with(|f| { live_in_count[f].try_into().unwrap() })
+    );
+
+    new_order.reverse();
+
+    Some(InstructionOrder { order: new_order })
+}
+
+struct ScheduleUnit {
+    block_idx: usize,
+    can_reorder: bool,
+
+    live_in_count: PerRegFile<u32>,
+    live_out: Option<LiveSet>,
+
+    instrs: Vec<Box<Instr>>,
+    new_order: Option<InstructionOrder>,
+    last_tried_schedule_type: Option<ScheduleType>,
+    peak_gpr_count: i32,
+}
+
+impl ScheduleUnit {
+    fn schedule(
+        &mut self,
+        sm: &dyn ShaderModel,
+        max_regs: PerRegFile<i32>,
+        schedule_type: ScheduleType,
+        thresholds: ScheduleThresholds,
+    ) {
+        assert!(self.can_reorder);
+        self.last_tried_schedule_type = Some(schedule_type);
+        let new_order = sched_buffer(
+            sm,
+            max_regs,
+            &self.instrs,
+            self.live_in_count,
+            self.live_out.as_ref().unwrap(),
+            thresholds,
+        );
+
+        if let Some(x) = new_order {
+            self.new_order = Some(x);
+        }
+    }
+}
+
+struct ScheduleUnits(Vec<ScheduleUnit>);
+
+impl ScheduleUnits {
+    fn new() -> Self {
+        ScheduleUnits(Vec::new())
+    }
+
+    fn push_instr(
+        &mut self,
+        instr: Box<Instr>,
+        block_idx: usize,
+        can_reorder: bool,
+        live_before_instr: &LiveSet,
+        max_regs: PerRegFile<i32>,
+    ) -> &Instr {
+        let current_usable = match self.0.last() {
+            Some(last) => {
+                last.block_idx == block_idx && last.can_reorder == can_reorder
+            }
+            None => false,
+        };
+        if !current_usable {
+            if let Some(last) = self.0.last_mut() {
+                if last.can_reorder && last.live_out.is_none() {
+                    assert_eq!(last.block_idx, block_idx);
+                    last.live_out = Some(live_before_instr.clone());
+                }
+            };
+            self.0.push(ScheduleUnit {
+                block_idx,
+                can_reorder,
+
+                live_in_count: PerRegFile::new_with(|f| {
+                    live_before_instr.count(f)
+                }),
+                live_out: None,
+
+                instrs: Vec::new(),
+                new_order: None,
+                last_tried_schedule_type: None,
+                peak_gpr_count: {
+                    let live_count = PerRegFile::new_with(|f| {
+                        live_before_instr.count(f).try_into().unwrap()
+                    });
+                    calc_used_gprs(live_count, max_regs)
+                },
+            });
+        }
+        let last = self.0.last_mut().unwrap();
+        last.instrs.push(instr);
+        last.instrs.last().unwrap()
+    }
+
+    fn update_gpr_count(&mut self, count: i32) {
+        let last = self.0.last_mut().unwrap();
+        last.peak_gpr_count = max(last.peak_gpr_count, count);
+    }
+
+    fn finish_block(&mut self, block_idx: usize, live_out: &LiveSet) {
+        let last = self.0.last_mut().unwrap();
+
+        if last.can_reorder {
+            assert!(last.live_out.is_none());
+            assert_eq!(last.block_idx, block_idx);
+            last.live_out = Some(live_out.clone());
+        }
+    }
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
+enum ScheduleType {
+    RegLimit(u8),
+    Spill,
+}
+
+impl ScheduleType {
+    fn thresholds(
+        &self,
+        max_regs: PerRegFile<i32>,
+        schedule_unit: &ScheduleUnit,
+    ) -> ScheduleThresholds {
+        match self {
+            ScheduleType::RegLimit(gpr_target) => ScheduleThresholds {
+                heuristic_threshold: i32::from(*gpr_target) - TARGET_FREE,
+                quit_threshold: i32::from(*gpr_target),
+            },
+            ScheduleType::Spill => ScheduleThresholds {
+                heuristic_threshold: max_regs[RegFile::GPR]
+                    - SW_RESERVED_GPRS_SPILL
+                    - TARGET_FREE,
+                quit_threshold: schedule_unit.peak_gpr_count,
+            },
+        }
+    }
+}
+
+fn get_schedule_types(
+    max_regs: PerRegFile<i32>,
+    min_gpr_target: i32,
+    max_gpr_target: i32,
+    reserved_gprs: i32,
+) -> Vec<ScheduleType> {
+    let mut out = Vec::new();
+
+    let mut gpr_target =
+        next_occupancy_cliff_with_reserved(min_gpr_target, reserved_gprs);
+    while gpr_target < max_regs[RegFile::GPR] {
+        out.push(ScheduleType::RegLimit(gpr_target.try_into().unwrap()));
+
+        // We want only 1 entry that's greater than or equal to the original
+        // schedule (it can be greater in cases where increasing the number of
+        // registers doesn't change occupancy)
+        if gpr_target >= max_gpr_target {
+            return out;
+        }
+
+        gpr_target =
+            next_occupancy_cliff_with_reserved(gpr_target + 1, reserved_gprs);
+    }
+
+    assert!(gpr_target >= max_regs[RegFile::GPR]);
+    out.push(ScheduleType::RegLimit(
+        (max_regs[RegFile::GPR] - SW_RESERVED_GPRS)
+            .try_into()
+            .unwrap(),
+    ));
+
+    // Only allow spilling if the original schedule spilled
+    if max_gpr_target > max_regs[RegFile::GPR] {
+        out.push(ScheduleType::Spill);
+    }
+    return out;
+}
+
+impl Function {
+    pub fn opt_instr_sched_prepass(
+        &mut self,
+        sm: &dyn ShaderModel,
+        max_regs: PerRegFile<i32>,
+    ) {
+        let liveness = SimpleLiveness::for_function(self);
+        let mut live_out_sets: Vec<LiveSet> = Vec::new();
+
+        #[cfg(debug_assertions)]
+        let orig_instr_counts: Vec<usize> =
+            self.blocks.iter().map(|b| b.instrs.len()).collect();
+
+        let reserved_gprs = SW_RESERVED_GPRS + (sm.hw_reserved_gprs() as i32);
+
+        // First pass: Set up data structures and gather some statistics about
+        // register pressure
+
+        // lower and upper bounds for how many gprs we will use
+        let mut min_gpr_target = 1;
+        let mut max_gpr_target = 1;
+
+        let mut schedule_units = ScheduleUnits::new();
+
+        for block_idx in 0..self.blocks.len() {
+            let block_live = liveness.block_live(block_idx);
+            let mut live_set = match self.blocks.pred_indices(block_idx) {
+                [] => LiveSet::new(),
+                [pred, ..] => LiveSet::from_iter(
+                    live_out_sets[*pred]
+                        .iter()
+                        .filter(|ssa| block_live.is_live_in(ssa))
+                        .cloned(),
+                ),
+            };
+
+            let block = &mut self.blocks[block_idx];
+
+            for (ip, instr) in
+                std::mem::take(&mut block.instrs).into_iter().enumerate()
+            {
+                let can_reorder = match side_effect_type(&instr.op) {
+                    SideEffect::None | SideEffect::Memory => true,
+                    SideEffect::Barrier => false,
+                };
+                let instr = schedule_units.push_instr(
+                    instr,
+                    block_idx,
+                    can_reorder,
+                    &live_set,
+                    max_regs,
+                );
+                let live_count =
+                    live_set.insert_instr_top_down(ip, instr, block_live);
+                let live_count =
+                    PerRegFile::new_with(|f| live_count[f].try_into().unwrap());
+                let used_gprs = calc_used_gprs(live_count, max_regs);
+                schedule_units.update_gpr_count(used_gprs);
+
+                // We never want our target to be worse than the original schedule
+                max_gpr_target = max(max_gpr_target, used_gprs);
+                if !can_reorder {
+                    // If we can't reorder an instruction, then it forms a lower
+                    // bound on how well we can do after rescheduling
+                    min_gpr_target = max(min_gpr_target, used_gprs);
+                }
+            }
+            schedule_units.finish_block(block_idx, &live_set);
+
+            live_out_sets.push(live_set);
+        }
+
+        // Second pass: Generate a schedule for each schedule_unit
+        let mut schedule_types = get_schedule_types(
+            max_regs,
+            min_gpr_target,
+            max_gpr_target,
+            reserved_gprs,
+        );
+        schedule_types.reverse();
+
+        for u in schedule_units.0.iter_mut() {
+            if !u.can_reorder {
+                continue;
+            }
+            loop {
+                let schedule_type = *schedule_types.last().unwrap();
+                let thresholds = schedule_type.thresholds(max_regs, u);
+
+                u.schedule(sm, max_regs, schedule_type, thresholds);
+
+                if u.new_order.is_some() {
+                    // Success!
+                    break;
+                }
+
+                if schedule_types.len() > 1 {
+                    // We've failed to schedule using the existing settings, so
+                    // switch to the next schedule type, which will have more
+                    // gprs
+                    schedule_types.pop();
+                } else {
+                    // No other schedule types to try - this implies that the
+                    // original program has a better instruction order than what
+                    // our heuristics can generate. Just keep the original
+                    // instruction order
+                    break;
+                }
+            }
+        }
+
+        // Third pass: Apply the generated schedules
+        let schedule_type = schedule_types.into_iter().last().unwrap();
+
+        for mut u in schedule_units.0.into_iter() {
+            let block = &mut self.blocks[u.block_idx];
+
+            // If the global register limit has increased, then we can schedule
+            // again with the new parameters
+            if u.can_reorder
+                && u.last_tried_schedule_type != Some(schedule_type)
+            {
+                let thresholds = schedule_type.thresholds(max_regs, &u);
+                u.schedule(sm, max_regs, schedule_type, thresholds);
+            }
+
+            match u.new_order {
+                Some(order) => block.instrs.extend(order.apply(u.instrs)),
+                None => block.instrs.extend(u.instrs.into_iter()),
+            }
+        }
+
+        debug_assert_eq!(
+            orig_instr_counts,
+            self.blocks
+                .iter()
+                .map(|b| b.instrs.len())
+                .collect::<Vec<usize>>()
+        );
+
+        if let ScheduleType::RegLimit(limit) = schedule_type {
+            // Our liveness calculations should ideally agree with SimpleLiveness
+            debug_assert!(
+                {
+                    let live = SimpleLiveness::for_function(self);
+                    let max_live = live.calc_max_live(self);
+                    max_live[RegFile::GPR]
+                } <= limit.into()
+            );
+        }
+    }
+}
+
+impl Shader<'_> {
+    /// Pre-RA instruction scheduling
+    ///
+    /// We prioritize:
+    /// 1. Occupancy
+    /// 2. Decreasing spills to memory
+    /// 3. Instruction level parallelism
+    ///
+    /// We accomplish this by having an outer loop that tries different register
+    /// limits in order of most to least occupancy. The inner loop computes
+    /// actual schedules using a heuristic inspired by Goodman & Hsu 1988
+    /// section 3, although the heuristic from that paper cannot be used
+    /// directly here because they assume a single register file and we have
+    /// multiple. Care is also taken to model quirks of register pressure on
+    /// NVIDIA GPUs corretly.
+    ///
+    /// J. R. Goodman and W.-C. Hsu. 1988. Code scheduling and register
+    ///     allocation in large basic blocks. In Proceedings of the 2nd
+    ///     international conference on Supercomputing (ICS '88). Association
+    ///     for Computing Machinery, New York, NY, USA, 442–452.
+    ///     https://doi.org/10.1145/55364.55407
+    pub fn opt_instr_sched_prepass(&mut self) {
+        let mut max_regs = PerRegFile::<i32>::new_with(|f| {
+            self.sm.num_regs(f).try_into().unwrap()
+        });
+        if let ShaderStageInfo::Compute(cs_info) = &self.info.stage {
+            max_regs[RegFile::GPR] = min(
+                max_regs[RegFile::GPR],
+                (gpr_limit_from_local_size(&cs_info.local_size)
+                    - self.sm.hw_reserved_gprs())
+                .try_into()
+                .unwrap(),
+            );
+        }
+        max_regs[RegFile::GPR] -= SW_RESERVED_GPRS;
+
+        for f in &mut self.functions {
+            f.opt_instr_sched_prepass(self.sm, max_regs);
+        }
+    }
+}
diff --git a/src/nouveau/compiler/nak/sched_common.rs b/src/nouveau/compiler/nak/sched_common.rs
new file mode 100644
index 0000000000000000000000000000000000000000..3920d9ca8e85167056873170c6ee34256116967d
--- /dev/null
+++ b/src/nouveau/compiler/nak/sched_common.rs
@@ -0,0 +1,226 @@
+// Copyright © 2022 Collabora, Ltd.
+// SPDX-License-Identifier: MIT
+
+use crate::ir::*;
+
+use std::ops::{Index, IndexMut, Range};
+
+pub fn exec_latency(sm: u8, op: &Op) -> u32 {
+    if sm >= 70 {
+        match op {
+            Op::Bar(_) | Op::MemBar(_) => {
+                if sm >= 80 {
+                    6
+                } else {
+                    5
+                }
+            }
+            Op::CCtl(_op) => {
+                // CCTL.C needs 8, CCTL.I needs 11
+                11
+            }
+            // Op::DepBar(_) => 4,
+            _ => 1, // TODO: co-issue
+        }
+    } else {
+        match op {
+            Op::CCtl(_)
+            | Op::MemBar(_)
+            | Op::Bra(_)
+            | Op::SSy(_)
+            | Op::Sync(_)
+            | Op::Brk(_)
+            | Op::PBk(_)
+            | Op::Cont(_)
+            | Op::PCnt(_)
+            | Op::Exit(_)
+            | Op::Bar(_)
+            | Op::Kill(_)
+            | Op::OutFinal(_) => 13,
+            _ => 1,
+        }
+    }
+}
+
+pub fn instr_latency(op: &Op, dst_idx: usize) -> u32 {
+    let file = match op.dsts_as_slice()[dst_idx] {
+        Dst::None => return 0,
+        Dst::SSA(vec) => vec.file().unwrap(),
+        Dst::Reg(reg) => reg.file(),
+    };
+
+    // This is BS and we know it
+    match file {
+        RegFile::GPR => 6,
+        RegFile::UGPR => 12,
+        RegFile::Pred => 13,
+        RegFile::UPred => 11,
+        RegFile::Bar => 0, // Barriers have a HW scoreboard
+        RegFile::Carry => 6,
+        RegFile::Mem => panic!("Not a register"),
+    }
+}
+
+/// Read-after-write latency
+pub fn raw_latency(
+    _sm: u8,
+    write: &Op,
+    dst_idx: usize,
+    _read: &Op,
+    _src_idx: usize,
+) -> u32 {
+    instr_latency(write, dst_idx)
+}
+
+/// Write-after-read latency
+pub fn war_latency(
+    _sm: u8,
+    _read: &Op,
+    _src_idx: usize,
+    _write: &Op,
+    _dst_idx: usize,
+) -> u32 {
+    // We assume the source gets read in the first 4 cycles.  We don't know how
+    // quickly the write will happen.  This is all a guess.
+    4
+}
+
+/// Write-after-write latency
+pub fn waw_latency(
+    _sm: u8,
+    a: &Op,
+    a_dst_idx: usize,
+    _b: &Op,
+    _b_dst_idx: usize,
+) -> u32 {
+    // We know our latencies are wrong so assume the wrote could happen anywhere
+    // between 0 and instr_latency(a) cycles
+    instr_latency(a, a_dst_idx)
+}
+
+/// Predicate read-after-write latency
+pub fn paw_latency(_sm: u8, _write: &Op, _dst_idx: usize) -> u32 {
+    13
+}
+
+pub struct RegTracker<T> {
+    reg: [T; 255],
+    ureg: [T; 63],
+    pred: [T; 7],
+    upred: [T; 7],
+    carry: [T; 1],
+}
+
+fn new_array_with<T, const N: usize>(f: &impl Fn() -> T) -> [T; N] {
+    let mut v = Vec::new();
+    for _ in 0..N {
+        v.push(f());
+    }
+    v.try_into()
+        .unwrap_or_else(|_| panic!("Array size mismatch"))
+}
+
+impl<T> RegTracker<T> {
+    pub fn new_with(f: &impl Fn() -> T) -> Self {
+        Self {
+            reg: new_array_with(f),
+            ureg: new_array_with(f),
+            pred: new_array_with(f),
+            upred: new_array_with(f),
+            carry: new_array_with(f),
+        }
+    }
+
+    pub fn for_each_instr_pred_mut(
+        &mut self,
+        instr: &Instr,
+        mut f: impl FnMut(&mut T),
+    ) {
+        if let PredRef::Reg(reg) = &instr.pred.pred_ref {
+            for i in &mut self[*reg] {
+                f(i);
+            }
+        }
+    }
+
+    pub fn for_each_instr_src_mut(
+        &mut self,
+        instr: &Instr,
+        mut f: impl FnMut(usize, &mut T),
+    ) {
+        for (i, src) in instr.srcs().iter().enumerate() {
+            match &src.src_ref {
+                SrcRef::Reg(reg) => {
+                    for t in &mut self[*reg] {
+                        f(i, t);
+                    }
+                }
+                SrcRef::CBuf(CBufRef {
+                    buf: CBuf::BindlessUGPR(reg),
+                    ..
+                }) => {
+                    for t in &mut self[*reg] {
+                        f(i, t);
+                    }
+                }
+                _ => (),
+            }
+        }
+    }
+
+    pub fn for_each_instr_dst_mut(
+        &mut self,
+        instr: &Instr,
+        mut f: impl FnMut(usize, &mut T),
+    ) {
+        for (i, dst) in instr.dsts().iter().enumerate() {
+            if let Dst::Reg(reg) = dst {
+                for t in &mut self[*reg] {
+                    f(i, t);
+                }
+            }
+        }
+    }
+}
+
+impl<T> Index<RegRef> for RegTracker<T> {
+    type Output = [T];
+
+    fn index(&self, reg: RegRef) -> &[T] {
+        let range = reg.idx_range();
+        let range = Range {
+            start: usize::try_from(range.start).unwrap(),
+            end: usize::try_from(range.end).unwrap(),
+        };
+
+        match reg.file() {
+            RegFile::GPR => &self.reg[range],
+            RegFile::UGPR => &self.ureg[range],
+            RegFile::Pred => &self.pred[range],
+            RegFile::UPred => &self.upred[range],
+            RegFile::Carry => &self.carry[range],
+            RegFile::Bar => &[], // Barriers have a HW scoreboard
+            RegFile::Mem => panic!("Not a register"),
+        }
+    }
+}
+
+impl<T> IndexMut<RegRef> for RegTracker<T> {
+    fn index_mut(&mut self, reg: RegRef) -> &mut [T] {
+        let range = reg.idx_range();
+        let range = Range {
+            start: usize::try_from(range.start).unwrap(),
+            end: usize::try_from(range.end).unwrap(),
+        };
+
+        match reg.file() {
+            RegFile::GPR => &mut self.reg[range],
+            RegFile::UGPR => &mut self.ureg[range],
+            RegFile::Pred => &mut self.pred[range],
+            RegFile::UPred => &mut self.upred[range],
+            RegFile::Carry => &mut self.carry[range],
+            RegFile::Bar => &mut [], // Barriers have a HW scoreboard
+            RegFile::Mem => panic!("Not a register"),
+        }
+    }
+}
diff --git a/src/nouveau/compiler/nak/spill_values.rs b/src/nouveau/compiler/nak/spill_values.rs
index 7263dc3e08ab8f7993afc74c71e54cafa42147ec..6c77eccf919019591fc7ff4b01f5700e9c521693 100644
--- a/src/nouveau/compiler/nak/spill_values.rs
+++ b/src/nouveau/compiler/nak/spill_values.rs
@@ -10,7 +10,7 @@ use crate::liveness::{
 };
 
 use compiler::bitset::BitSet;
-use std::cell::RefCell;
+use std::cell::{Cell, RefCell};
 use std::cmp::{max, Ordering, Reverse};
 use std::collections::{BinaryHeap, HashMap, HashSet};
 
@@ -231,6 +231,67 @@ impl Spill for SpillGPR {
     }
 }
 
+struct SpillStatistics<'a, S>
+where
+    S: Spill,
+{
+    inner: S,
+    info: &'a mut ShaderInfo,
+    dst_is_mem: bool,
+    spill_count: Cell<u32>,
+    fill_count: Cell<u32>,
+}
+
+impl<'a, S> SpillStatistics<'a, S>
+where
+    S: Spill,
+{
+    fn new(inner: S, file: RegFile, info: &'a mut ShaderInfo) -> Self {
+        let dst_is_mem = inner.spill_file(file) == RegFile::Mem;
+        SpillStatistics {
+            inner,
+            info,
+            dst_is_mem,
+            spill_count: Cell::new(0),
+            fill_count: Cell::new(0),
+        }
+    }
+}
+
+impl<'a, S> Spill for SpillStatistics<'a, S>
+where
+    S: Spill,
+{
+    fn spill_file(&self, file: RegFile) -> RegFile {
+        self.inner.spill_file(file)
+    }
+
+    fn spill(&self, dst: SSAValue, src: Src) -> Box<Instr> {
+        self.spill_count.set(self.spill_count.get() + 1);
+        self.inner.spill(dst, src)
+    }
+
+    fn fill(&self, dst: Dst, src: SSAValue) -> Box<Instr> {
+        self.fill_count.set(self.fill_count.get() + 1);
+        self.inner.fill(dst, src)
+    }
+}
+
+impl<'a, S> Drop for SpillStatistics<'a, S>
+where
+    S: Spill,
+{
+    fn drop(&mut self) {
+        if self.dst_is_mem {
+            self.info.num_spills_to_mem += self.spill_count.get();
+            self.info.num_fills_from_mem += self.fill_count.get();
+        } else {
+            self.info.num_spills_to_reg += self.spill_count.get();
+            self.info.num_fills_from_reg += self.fill_count.get();
+        }
+    }
+}
+
 #[derive(Eq, PartialEq)]
 struct SSANextUse {
     ssa: SSAValue,
@@ -399,10 +460,12 @@ fn spill_values<S: Spill>(
     file: RegFile,
     limit: u32,
     spill: S,
+    info: &mut ShaderInfo,
 ) {
     let files = RegFileSet::from_iter([file]);
     let live = NextUseLiveness::for_function(func, &files);
     let blocks = &mut func.blocks;
+    let spill = SpillStatistics::new(spill, file, info);
 
     // Record the set of SSA values used within each loop
     let mut phi_dst_maps = Vec::new();
@@ -1023,27 +1086,32 @@ impl Function {
     /// just for the sake of a parallel copy.  While this may not be true in
     /// general, especially not when spilling to memory, the register allocator
     /// is good at eliding unnecessary copies.
-    pub fn spill_values(&mut self, file: RegFile, limit: u32) {
+    pub fn spill_values(
+        &mut self,
+        file: RegFile,
+        limit: u32,
+        info: &mut ShaderInfo,
+    ) {
         match file {
             RegFile::GPR => {
                 let spill = SpillGPR::new();
-                spill_values(self, file, limit, spill);
+                spill_values(self, file, limit, spill, info);
             }
             RegFile::UGPR => {
                 let spill = SpillUniform::new();
-                spill_values(self, file, limit, spill);
+                spill_values(self, file, limit, spill, info);
             }
             RegFile::Pred => {
                 let spill = SpillPred::new();
-                spill_values(self, file, limit, spill);
+                spill_values(self, file, limit, spill, info);
             }
             RegFile::UPred => {
                 let spill = SpillPred::new();
-                spill_values(self, file, limit, spill);
+                spill_values(self, file, limit, spill, info);
             }
             RegFile::Bar => {
                 let spill = SpillBar::new();
-                spill_values(self, file, limit, spill);
+                spill_values(self, file, limit, spill, info);
             }
             _ => panic!("Don't know how to spill {} registers", file),
         }
diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index 1bf195cfbf7cebc34e5bc1aa535dc46cd2f4bae0..f99d62c05cfd821a8c75f05651df43fd5a3a4499 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -1238,6 +1238,51 @@ nvk_shader_get_executable_statistics(
       stat->value.u64 = shader->info.num_instrs;
    }
 
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Static cycle count");
+      WRITE_STR(stat->description,
+                "Total cycles used by fixed-latency instructions in this shader");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.num_static_cycles;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Spills to memory");
+      WRITE_STR(stat->description, "Number of spills from GPRs to memory");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.num_spills_to_mem;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Fills from memory");
+      WRITE_STR(stat->description, "Number of fills from memory to GPRs");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.num_spills_to_mem;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Spills to reg");
+      WRITE_STR(stat->description,
+                "Number of spills between different register files");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.num_spills_to_reg;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Fills from reg");
+      WRITE_STR(stat->description,
+                "Number of fills between different register files");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.num_fills_from_reg;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Occupancy (warps/SM)");
+      WRITE_STR(stat->description, "Maximum number of warps per SM");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.occupancy_in_warps_per_sm;
+   }
+
    vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
       WRITE_STR(stat->name, "Code Size");
       WRITE_STR(stat->description,
